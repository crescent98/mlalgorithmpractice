{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on  \n",
    "https://towardsdatascience.com/linear-regression-from-scratch-with-tensorflow-2-part-1-3e2443804df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleLinearRegression:\n",
    "    def __init__(self, initializer='random'):\n",
    "        if initializer=='ones':\n",
    "            self.var = 1.\n",
    "        elif initializer=='zeros':\n",
    "            self.var = 0.\n",
    "        elif initializer=='random':\n",
    "            selfx.var = tf.random.uniform(shape=[], minval=0., maxval=1.)\n",
    "            \n",
    "        self.m = tf.Variable(1., shape=tf.TensorShape(None))\n",
    "        self.b = tf.Variable(self.var)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return tf.reduce_sum(self.m * x, 1) + self.b\n",
    "    \n",
    "    \n",
    "    # Loss Function    \n",
    "    def mse(self, true, predicted):\n",
    "        return tf.reduce_mean(tf.square(true-predicted))\n",
    "    \n",
    "    \n",
    "    # Updating the parameters\n",
    "    def update(self, X, y, learning_rate):\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            loss = self.mse(y, self.predict(X))\n",
    "            \n",
    "        print(\"Loss: \", loss)\n",
    "\n",
    "        dy_dm = g.gradient(loss, self.m)\n",
    "        dy_db = g.gradient(loss, self.b)\n",
    "        \n",
    "        self.m.assign_sub(learning_rate * dy_dm)\n",
    "        self.b.assign_sub(learning_rate * dy_db)\n",
    "        \n",
    "    # Train the method\n",
    "    def train(self, X, y, learning_rate=0.01, epochs=5):\n",
    "        \n",
    "        if len(X.shape)==1:\n",
    "            X=tf.reshape(X,[X.shape[0],1])\n",
    "        \n",
    "        self.m.assign([self.var]*X.shape[-1])\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            print(\"Epoch: \", i)\n",
    "            \n",
    "            self.update(X, y, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearRegression:\n",
    "    def __init__(self,initializer='random'):\n",
    "        if initializer=='ones':\n",
    "            self.var=1.\n",
    "        elif initializer=='zeros':\n",
    "            self.var=0\n",
    "        elif initializer=='random':\n",
    "            self.var=tf.random.uniform(shape=[],minval=0.,maxval=1.)\n",
    "        \n",
    "        self.m=tf.Variable(1., shape=tf.TensorShape(None))\n",
    "        self.b=tf.Variable(self.var)\n",
    "    def mse(self, true, predicted):\n",
    "        return tf.reduce_mean(tf.square(true-predicted))\n",
    "    \n",
    "    def predict(self,x):  \n",
    "        return tf.reduce_sum(self.m*x,1)+self.b\n",
    "    \n",
    "    def update(self,X,y,learning_rate):\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            loss=self.mse(y,self.predict(X))\n",
    "        \n",
    "        print(\"Loss: \",loss)\n",
    "    \n",
    "        dy_dm=g.gradient(loss,self.m)\n",
    "        dy_db=g.gradient(loss,self.b)\n",
    "        \n",
    "        self.m.assign_sub(learning_rate*dy_dm)\n",
    "        self.b.assign_sub(learning_rate*dy_db)\n",
    "        \n",
    "    def train(self,X,y,learning_rate=0.01,epochs=5):\n",
    "        if len(X.shape)==1:\n",
    "            X=tf.reshape(X,[X.shape[0],1])        \n",
    "        self.m.assign([self.var]*X.shape[-1])       \n",
    "        for i in range(epochs):\n",
    "            print(\"Epoch: \",i)       \n",
    "            self.update(X,y,learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test)=boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_label = y_train.mean(axis=0)\n",
    "std_label = y_train.std(axis=0)\n",
    "mean_feat = x_train.mean(axis=0)\n",
    "std_feat = x_train.std(axis=0)\n",
    "x_train = (x_train-mean_feat)/std_feat\n",
    "y_train = (y_train-mean_label)/std_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Loss:  Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RefVariable' object has no attribute '_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-384f7c5e3667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlinear_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-f1b799ffb797>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-f1b799ffb797>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, X, y, learning_rate)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mdy_dm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mdy_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RefVariable' object has no attribute '_id'"
     ]
    }
   ],
   "source": [
    "linear_model = SimpleLinearRegression('zeros')\n",
    "linear_model.train(x_train_norm, y_train_norm, learning_rate=0.1, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
